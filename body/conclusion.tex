\hspace{24pt}

This chapter will summarize the findings and contributions and further discuss some of the potential improvements to the paper. We will summarize the results and contributions in Section~\ref{sec:conclusion}. After that, we propose a series of methods that can be further studied or improved as our future work in Section~\ref{sec:future_work}.

\section{Conclusion} \label{sec:conclusion}

This paper intended to improve the neural machine translation between Chinese and Japanese by applying feature engineering to texts, constructing a word embedding with phonetic information, and combining the phonetic embedding with general word embedding that possesses semantics. The resulting joint semantic-phonetic embedding gave positive feedback in the neural machine translation model and the embedding analysis. 

In our translation task based on different models and tokenization methods, the use of joint embedding had achieved better BLEU scores than the use of semantic or phonetic embedding. Moreover, the translation results generated by the best model with joint embedding also revealed four advantages. These translations selected the correct, better text; and preserved Katakana and English text for the target sentences. As a result, these advantages have a positive connection to the joint embedding with the addition of phonetic information.

Embedding analysis further identified the effectiveness of joint embedding. The tests from analogical reasoning and outlier detection demonstrated that the joint embedding not only had retained the capabilities but also enhanced the performance of the semantic embedding. Furthermore, the similarity distance and Pearson correlation of synonyms, homonyms, and heteronyms were observed to be better in joint embedding, which indicates the improvement in semantics and noise resistance.

The success of translation task and embedding analysis revealed that the joint semantic-phonetic embedding has a certain degree of enhancement and contribution to the Chinese-Japanese neural machine translation system.

\section{Future Work} \label{sec:future_work}

A variety of research can be further investigated based on the method of this paper. We are going to list a few techniques and approaches that may further improve the existing performance.

\begin{itemize}
    \item Using any basic attempts to refine the research. For instance, using other tokenization methods, phonetic extraction methods, NMT models, and embedding training methods.
    \item Referring to the model for training Chinese word embedding as described in Section~\ref{sec:rw_cwe}. Training an embedding by combining phonetic information with Chinese characters and various sub-character features such as strokes and radicals.
    \item Using ELMO or BERT as the base embedding framework. Since they are both embeddings that require a binding model, it is possible to consider training a semantic model and a phonetic model and combining the two.
\end{itemize}
