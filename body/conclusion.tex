\hspace{24pt}

This chapter will summarize the findings and contributions and further discuss some of the potential improvements to the paper. We will summarize the results and contributions in Section~\ref{sec:conclusion}. After that, we propose a series of methods that can be further studied or improved as our future work in Section~\ref{sec:future_work}.

\section{Conclusion} \label{sec:conclusion}

This paper intended to improve the neural machine translation between Chinese and Japanese by applying feature engineering to texts, constructing a word embedding with phonetic information, and combining the phonetic embedding with general word embedding that possesses semantics. The resulting joint semantic-phonetic embedding gave positive feedback in the neural machine translation model and the embedding analysis. 

In our translation task based on different models and tokenization methods, the use of joint embedding had achieved better BLEU scores than the use of semantic or phonetic embedding. Moreover, the translation results generated by the best model with joint embedding also revealed four advantages. These translations selected the correct, better text; and preserved Katakana and English text for the target sentences. As a result, these advantages have a positive connection to the joint embedding with the addition of phonetic information.

Embedding analysis further identified the effectiveness of joint embedding. The tests from analogical reasoning and outlier detection demonstrated that the joint embedding not only had retained the capabilities but also enhanced the performance of the semantic embedding. Furthermore, the similarity distance and Pearson correlation of synonyms, homonyms, and heteronyms were observed to be better in joint embedding, which indicates the improvement in semantics and noise resistance.

The success of translation task and embedding analysis revealed that the joint semantic-phonetic embedding has a certain degree of enhancement and contribution to the Chinese-Japanese neural machine translation system.

\section{Future Work} \label{sec:future_work}

% 本論文有許多能夠深入研究的方向，我們在此列出幾個方法，這些方法有的或許能進一步分析現有表現，有的或許有機會能進一步提高表現。

% 改善產生聲音資訊的方法，例如使用其他的插件來產生並比較優劣，舉例

% 採用在 related work 中提到訓練 embedding 的模式，將聲音資訊與中文字、及各種 subcharacter feature 例如筆畫、部首來一起訓練一個 embedding。

% 採用 ELMO、BERT 當作基礎的 embedding 架構。由於他們兩者都是屬於需要綁定模型的 embedding，所以可以考慮訓練一個 semantic model 和一個 phonetic model 並將兩者結合起來。

% 最新研究展示 cnn 用於 NLP 領域強於 transformer。所以我們也可以試用 CONVS2S 當作模型架構，。