\hspace{24pt}

This chapter will summarize the findings and contributions and further discuss some potential future improvements.  We will summarize the results and contributions in Section~\ref{sec:conclusion}. After that, we propose a series of methods that can be further studied or improved as our future work in Section~\ref{sec:future_work}.

\section{Conclusion} \label{sec:conclusion}

Our work aimed to improve the neural machine translation between Chinese and Japanese by applying feature engineering to texts, constructing a word embedding with phonetic information, and combining the phonetic embedding with general word embedding that represents semantics. The resulting joint semantic-phonetic embedding obtained positive results in the neural machine translation model and the embedding analysis. 

We compared translation performance using different models and tokenization methods.
The use of joint embedding achieved better BLEU scores than the use of semantic or phonetic embedding.
Moreover, compared to semantic embedding, the translations generated by the best model with joint embedding tended to be more natural and grammatical, and showed the ability to utilize Western loanwords (written in katakana) in Japanese translations while faithfully preserving English acronyms and jargon.

Embedding analysis further identified the effectiveness of joint embedding. The tests from analogical reasoning and outlier detection demonstrated that the joint embedding not only had retained the capabilities but also enhanced the performance of the semantic embedding. Furthermore, the similarity distance and Pearson correlation of synonyms, homonyms, and heteronyms were observed to be better in joint embedding, indicating improvement in semantics and noise resistance.

Together, our results and analysis reveal demonstrate that joint semantic-phonetic embedding is a highly promising technique to improve Chinese-Japanese neural machine translation systems.

\section{Future Work} \label{sec:future_work}

A variety of research can be further investigated based on the method of this paper.
Here we list a few techniques and approaches that may further improve performance.

\begin{itemize}
    \item Using any basic attempts to refine the research. For instance, using other tokenization methods, phonetic extraction methods, NMT models, and embedding training methods.
    \item Referring to the model for training Chinese word embedding as described in Section~\ref{sec:rw_cwe}. Training an embedding by combining phonetic information with Chinese characters and various sub-character features such as strokes and radicals.
    \item Using ELMo or BERT as the base embedding framework. Since they are both embeddings that require a binding model, it is possible to consider training a semantic model and a phonetic model and combining the two.
\end{itemize}
