\hspace{24pt}

This chapter explains the details of the experiments. It includes the use of datasets (Section~\ref{sec:dataset}), the experimental environment (Section~\ref{sec:environment}), the parameters of the NMT model (Section~\ref{sec:parameter}), and the metrics (BLEU score) of the NMT model (Section~\ref{sec:metric}). Lastly, the BLEU scores of the translation system based on different tokenizations and word embeddings are shown in Section~\ref{sec:result}.

\section{Dataset} \label{sec:dataset}

Our datasets are acquired from the Asian Scientific Paper Excerpt Corpus (ASPEC) \cite{nakazawa-etal-2016-aspec} provided by the Japan Science and Technology Agency (JST) and the National Institute of Information and Communications Technology (NICT). ASPEC consists of a Japanese-English corpus (ASPEC-JE) with 3 million parallel sentences and a Japanese-Chinese corpus (ASPEC-JC) with 680,000 pairs.

We have selected ASPEC-JC as the dataset for our zh-ja NMT system, and the structure of the dataset is shown in Table~\ref{tab:aspec-jc}. ASPEC-JC is constructed by manually translating the excerpts of Japanese scientific papers into Chinese. Papers used for translation are derived from the Japan Science and Technology Agency (JST) or the Japan Science and Technology Information Aggregator, Electronic (​J-STAGE).

\vspace{0.4cm}
\begin{table}[h]
    \centering
    \begin{tabularx}{\textwidth}{bbb}\toprule
        Data Type & File Name & Number of sentences \\\midrule
        Train & train.txt & 672,315 \\
        Validation & dev.txt & 2,090 \\
        Validation-Test & devtest.txt & 2,148 \\
        Test & test.txt & 2,107 \\
        \bottomrule
    \end{tabularx}
    \caption{ASPEC-JC dataset}
    \label{tab:aspec-jc}
\end{table}

\section{Environment} \label{sec:environment}

We use PyTorch \footnote{https://pytorch.org/} and PyTorch Lightning \footnote{https://www.pytorchlightning.ai/} \cite{falcon2019pytorch}  as the main framework for constructing the entire NMT model (Section~\ref{sec:lightning}), as well as Weights \& Biases (wandb) \footnote{https://wandb.ai/site} \cite{wandb} for logging models, metrics, and dataflow (Section~\ref{sec:wandb}).

\subsection{PyTorch and PyTorch Lightning} \label{sec:lightning}

PyTorch has been widely used in academic research and production. We use PyTorch as a framework for processing data and designing the components (i.e., encoder, attention, decoder) of RNN and Transformer.

PyTorch-Lightning offers many advantages as a wrapper for PyTorch: Dataflow can be integrated more efficiently in \pythoninline{LightningDataModule}; the main source code, including the components designed by PyTorch, can be integrated into \pythoninline{LightningModule}; using \pythoninline{Trainer} can directly utilize the \pythoninline{LightningDataModule} and \pythoninline{LightningModule}, and help us use the correct engineering code such as gradient update, data transfer between CPU and GPU. In addition, PyTorch-Lightning provides a lot of plugins to simplify research, such as multi-GPU or TPU training, checkpointing and logging of models, implementing 16-bit precision, early stopping, finding learning rate and batch size, etc.

\subsection{Weights and Biases (wandb)} \label{sec:wandb}

Wandb, similar to Tensorboard, is a powerful tool for logging model training and is used by OpenAI \footnote{https://wandb.ai/site/articles/why-experiment-tracking-is-crucial-to-openai}, the company designed GPT-3. Wandb can visualize all training metrics and system performance, record hyperparameters used in the experiments for easy replicating, integrate dataset in the cloud to facilitate the advantages of cross-platform. In the experiments, we use the \pythoninline{WandbLogger} provided by PyTorch-Lightning to integrate wandb and record the experimental results of all models, which will be presented in Section~\ref{sec:result}.

\section{Parameter} \label{sec:parameter}

We implement eight (two-by-four) experiments for RNN and Transformer experiments. The experiment uses two different methods for tokenization. one is to use SentencePiece for both Chinese and Japanese, and the other is to use Jieba and Janome for Chinese and Japanese. Every experiment validates four scenarios, which are the model without any pre-trained embedding (i.e., the baseline), the model using only semantic embedding, the model using only phonetic embedding, and the model using joint semantic-phonetic embedding.

Some parameters are identical in all experiments. The dataset is 50,000 sentence pairs which filtered (described in Section~\ref{sec:corpus_filtering}) and sampled by Python \pythoninline{random} module. The dictionary size for tokenizer and embedding is fixed to 32,000 and the dimension of embedding is fixed to 300. The other hyperparameters are listed in Table~\ref{tab:hyperparameters}.

\vspace{0.4cm}
\begin{table}[h]
    \centering
    \begin{tabularx}{\textwidth}{bbb}\toprule
        Hyperparameters & RNN & Transformer \\\midrule
        dropout & 0.1 & 0.1 \\
        hidden\_dim (pf\_dim) & 512 & 512 \\
        learning rate & 7e-4 & 5e-4 \\
        precision & 16 & 32 \\
        attention heads & - & 6 \\
        layers & 1 & 3 \\ 
        \bottomrule
    \end{tabularx}
    \caption{The hyperparameters}
    \label{tab:hyperparameters}
\end{table}

Catastrophic forgetting \cite{kirkpatrick2017overcoming} may occur during training with pre-trained embedding. In other words, the gradients in the first few batches will drastically change the embeddings and lose the initial meaning from vectors completely. We overcome the problem by freezing the weights of embeddings in the first $n$ epochs, and fine-tuning the embeddings in the subsequent epochs. Therefore, $n$ is also a hyperparameter of RNN and Transformer models. We apply $n=3$ to RNN and $n=1$ to Transformer to achieve the best results.

\section{Metric} \label{sec:metric}

We evaluated our zh-ja NMT system using the BLEU (Bilingual Evaluation Understudy) score \cite{papineni2002bleu}. The BLEU score is a number from 0 to 100 that measures the similarity between translated sentences from model and high-quality reference translations. A score of 0 means there is no overlap between machine translation and reference translation, and a score of 100 means a complete overlap between the two. A common interpretation of the BLEU score can be referred to Table~\ref{tab:bleu_score_interpretation}.

\newpage

\vspace{0.4cm}
\begin{table}[h]
    \centering
    \begin{tabularx}{\textwidth}{p{3cm}b}\toprule
        BLEU Score & Interpretation \\\midrule
        $< 10$ & Almost useless \\
        $10 - 19$ & Hard to get the gist \\
        $20 - 29$ & The gist is clear, but has significant grammatical errors \\
        $30 - 40$ & Understandable to good translations \\
        $40 - 50$ & High quality translations \\
        $50 - 60$ & Very high quality, adequate, and fluent translations \\
        $> 60$ & Quality often better than human \\
    \end{tabularx}
    \caption[Interpretation of the BLEU score]{Interpretation of the BLEU score\protect\footnotemark}
    \label{tab:bleu_score_interpretation}
\end{table}

\footnotetext{Source: https://www.cs.cmu.edu/~alavie/Presentations/MT-Evaluation-MT-Summit-Tutorial-19Sep11.pdf}

Mathematically, the BLEU score is defined as eq. \ref{eq:eq16} and eq. \ref{eq:eq17}. The formula consists of two parts: the brevity penalty and the n-gram overlap. The brevity penalty penalizes the sentences that were translated too short, which were easier to score higher because of the lower denominator (in eq. \ref{eq:eq17}) calculated by n-gram. The n-gram overlap counts the number of overlaps between the translated sentences and the corresponding unigrams, bigrams, trigrams, and four-grams in the reference translation ($\text{Count}_\text{clip}$). Unigrams account for the adequacy and longer n-grams account for the fluency of the translation.

\begin{equation}
    \mathrm{BLEU}=\underbrace{\min \left(1, \exp \left(1-\frac{\text { reference-length }}{\text { output-length }}\right)\right)}_{\text {brevity penalty }} \underbrace{\left(\prod_{i=1}^{4} \text { precision }_{i}\right)^{1 / 4}}_{\text {n-gram overlap }} \label{eq:eq16}
\end{equation}

\begin{equation}
    \text { precision }_{n} = \frac{\sum_{ \text {n-gram} \in C} \text {Count}_{\text {clip}}(\text {n-gram})}{\sum_{\text{n-gram}^{\prime} \in C^{\prime}} \operatorname{Count}\left(\text{n-gram}^{\prime}\right)} \label{eq:eq17}
\end{equation}

Here are several considerations for evaluating translation models using the BLEU score. First, BLEU is a corpus-based metric that requires evaluating multiple sentences (corpus) at once; the score for individual sentences will lose some meanings; in other words, statistics are accumulated across the sentences when computing the score. Second, BLEU cannot distinguish the errors between the function words and the content words; the dropped function words such as ``a, the, of'' carry the same penalties as important content words that are mistranslated or lost. Last, BLEU is not good at capturing the meaning and grammaticality of sentences; namely, the single word that can change the semantics such as ``not'' or the long-range dependencies that are greater than the length of the n-gram will be underestimated.

\section{Result} \label{sec:result}

% 我們以 WAT 2020 做為標準。什麼是 WAT 2020。他們同樣使用 ASPEC。分別用什麼分詞。分數為多少。

% RNN 的結果，SENTENCEPIECE 結果，LOSS，BLEU。JIEBA JANOME 結果，LOSS，BLEU。

% TRANSFORMER 的結果，SENTENCEPIECE 結果，LOSS，BLEU。JIEBA JANOME 結果，LOSS，BLEU。


% Sample
% \begin{center}\vspace{0.3cm}
%     \begin{tabular}%
%       {l%
%        P{6cm}%
%        P{6cm}%
%        P{6cm}%
%        P{6cm}} \toprule
%     & \multicolumn{2}{c}{Sentencepiece} & \multicolumn{2}{c}{Jieba \& Janome}
%     \\\cmidrule(l){2-3}\cmidrule(l){4-5}
%     & RNN & Transformer & RNN & Transformer\\\midrule
%     w/o Pre-trained emb  & 21.63 & 24.32 & 25.16 & 29.31 \\ \midrule
%     w/ Semantic emb  & 21.66 & 25.72 & 26.71 & 31.23  \\ \midrule
%     w/ Phonetic emb & 21.32 & 23.48 & 26.18 & 30.9 \\\midrule
%     w/ Joint emb & \textbf{22.33} & \textbf{26.44} & \textbf{27.05} & \textbf{32.48}\\
%     \bottomrule
%     \end{tabular}
%     \captionof{table}{\color{Green}BLEU Scores on Sampled Data}
%     \label{table:bleu_sample}
% \end{center}
    

% WAT
% \begin{center}\vspace{0.3cm}
% \begin{tabular}%
%     {l%
%   P{4cm}%
%   P{7cm}%
%   P{4.5cm}%
%   P{4.5cm}%
%   P{4.5cm}} \toprule
%   &WAT2020 & w/o Pre-trained & w/ Semantic & w/ Phonetic & w/ Joint \\\midrule
%   Baseline & 47.00 &&& \\
%   Our Best Model && 52.78 & 52.83 & 53.04 & \textbf{53.13}\\
% \end{tabular}
% \captionof{table}{\color{Green}BLEU Scores on Full Well-Filtered Dataset}
% \label{table:bleu_full}
% \end{center}
